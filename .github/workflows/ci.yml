# Continuous integration
name: CI

on:
  pull_request:
    types: [opened, reopened, synchronize]
    paths:
      - "sleap_roots/**"
      - "tests/**"
      - ".github/workflows/ci.yml"
      - "pyproject.toml"
      - "uv.lock"
      - ".python-version"
  push:
    branches:
      - main
      - infra/*
    paths:
      - "sleap_roots/**"
      - "tests/**"
      - ".github/workflows/**"
      - "pyproject.toml"
      - "uv.lock"
      - ".python-version"

# Cancel in-progress runs when a new commit is pushed to the same branch
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  lint:
    name: Lint (Black + pydocstyle)
    runs-on: ubuntu-22.04
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          github-token: ${{ secrets.GITHUB_TOKEN }}
          cache-dependency-glob: "**/uv.lock"

      - name: Install Python from .python-version
        run: uv python install

      - name: Sync deps
        run: uv sync --frozen

      - name: Run Black
        run: uv run black --check sleap_roots tests

      - name: Run pydocstyle
        run: uv run pydocstyle --convention=google sleap_roots/

  test:
    name: Test (${{ matrix.os }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-22.04, windows-2022, macos-14]

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true  # Fetch large files with Git LFS

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          github-token: ${{ secrets.GITHUB_TOKEN }}
          cache-dependency-glob: "**/uv.lock"

      - name: Install Python from .python-version
        run: uv python install

      - name: Sync deps (runtime + dev group)
        run: uv sync --frozen

      - name: Environment info
        run: |
          uv --version
          uv run python -c "import sys, platform; print(sys.version); print(platform.platform())"
          uv tree

      - name: Test with pytest
        if: ${{ !startsWith(matrix.os, 'ubuntu') }}
        run: uv run pytest tests/

      - name: Test with pytest (with coverage)
        if: ${{ startsWith(matrix.os, 'ubuntu') }}
        run: uv run pytest --cov=sleap_roots --cov-report=xml --cov-report=term-missing tests/

      - name: Upload coverage
        uses: codecov/codecov-action@v4
        if: ${{ startsWith(matrix.os, 'ubuntu') }}
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
          fail_ci_if_error: true
          verbose: false

  benchmark:
    name: Performance Benchmarks
    runs-on: ubuntu-22.04
    # Only run benchmarks on pushes to main to avoid noise during development
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          lfs: true  # Fetch large test data files

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          github-token: ${{ secrets.GITHUB_TOKEN }}
          cache-dependency-glob: "**/uv.lock"

      - name: Install Python from .python-version
        run: uv python install

      - name: Sync deps (runtime + dev group)
        run: uv sync --frozen

      - name: Run benchmarks
        run: |
          uv run pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=min,max,mean,stddev,median \
            --benchmark-sort=name

      - name: Display benchmark results
        if: always()
        run: |
          echo "=== Benchmark Results ==="
          if [ -f benchmark-results.json ]; then
            uv run python -c "
          import json
          with open('benchmark-results.json') as f:
              data = json.load(f)
              for bench in data['benchmarks']:
                  name = bench['name']
                  stats = bench['stats']
                  print(f'{name}:')
                  print(f'  Mean: {stats[\"mean\"]:.4f}s Â± {stats[\"stddev\"]:.4f}s')
                  print(f'  Min:  {stats[\"min\"]:.4f}s')
                  print(f'  Max:  {stats[\"max\"]:.4f}s')
                  print()
          "
          fi

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: benchmark-results
          path: benchmark-results.json
          retention-days: 30

      - name: Store baseline
        if: success()
        run: |
          mkdir -p .benchmarks/baselines .benchmarks/history

          # Store as main baseline
          cp benchmark-results.json .benchmarks/baselines/main.json

          # Store with commit SHA for history
          cp benchmark-results.json .benchmarks/baselines/${{ github.sha }}.json

          # Store daily aggregate
          DATE=$(date +%Y-%m-%d)
          cp benchmark-results.json .benchmarks/history/${DATE}.json

          # Cleanup old baselines (>90 days)
          find .benchmarks/baselines -name "*.json" -type f -mtime +90 ! -name "main.json" -delete 2>/dev/null || true
          find .benchmarks/history -name "*.json" -type f -mtime +90 -delete 2>/dev/null || true

      - name: Commit baseline to repo
        if: success()
        run: |
          git config user.name "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add -f .benchmarks/
          git commit -m "chore: update benchmark baselines [skip ci]" || echo "No changes to commit"
          git push || echo "Nothing to push"

  benchmark-pr:
    name: PR Benchmark Comparison
    runs-on: ubuntu-22.04
    # Run on PRs to compare against main baseline
    if: github.event_name == 'pull_request'
    permissions:
      contents: read
      pull-requests: write

    env:
      BENCHMARK_MAX_REGRESSION: 0.15  # 15% regression threshold

    steps:
      - name: Checkout PR branch
        uses: actions/checkout@v4
        with:
          lfs: true
          fetch-depth: 0  # Need full history to access main branch

      - name: Set up uv
        uses: astral-sh/setup-uv@v5
        with:
          enable-cache: true
          github-token: ${{ secrets.GITHUB_TOKEN }}
          cache-dependency-glob: "**/uv.lock"

      - name: Install Python from .python-version
        run: uv python install

      - name: Sync deps (runtime + dev group)
        run: uv sync --frozen

      - name: Fetch baseline from main
        id: baseline
        run: |
          # Try to get baseline from main branch
          git fetch origin main
          if git show origin/main:.benchmarks/baselines/main.json > baseline.json 2>/dev/null; then
            echo "baseline_exists=true" >> $GITHUB_OUTPUT
            echo "Baseline found from main branch"
          else
            echo "baseline_exists=false" >> $GITHUB_OUTPUT
            echo "No baseline found - this is expected for new benchmark setup"
          fi

      - name: Run benchmarks
        run: |
          uv run pytest tests/benchmarks/ \
            --benchmark-only \
            --benchmark-json=benchmark-results.json \
            --benchmark-columns=min,max,mean,stddev,median \
            --benchmark-sort=name

      - name: Compare benchmarks
        id: compare
        run: |
          python .claude/scripts/compare-benchmarks.py \
            --baseline baseline.json \
            --current benchmark-results.json \
            --threshold $BENCHMARK_MAX_REGRESSION \
            > benchmark-comparison.md 2>&1 || true

          # Check for regressions
          if grep -q "âš ï¸ Performance Regressions Detected" benchmark-comparison.md; then
            echo "has_regression=true" >> $GITHUB_OUTPUT
          else
            echo "has_regression=false" >> $GITHUB_OUTPUT
          fi

      - name: Post PR comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const comparison = fs.readFileSync('benchmark-comparison.md', 'utf8');

            // Find existing benchmark comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number
            });

            const botComment = comments.find(comment =>
              comment.user.type === 'Bot' &&
              comment.body.includes('ðŸ“Š Benchmark Results')
            );

            const body = comparison;

            if (botComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body
              });
            }

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pr-benchmark-results
          path: |
            benchmark-results.json
            benchmark-comparison.md
          retention-days: 30

      - name: Fail on regression
        if: steps.compare.outputs.has_regression == 'true'
        run: |
          echo "::error::Performance regression detected! See PR comment for details."
          echo "Regressions exceed the ${BENCHMARK_MAX_REGRESSION} threshold."
          exit 1